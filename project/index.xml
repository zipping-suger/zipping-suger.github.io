<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Yixiao Li</title>
    <link>https://zipping-suger.github.io/project/</link>
      <atom:link href="https://zipping-suger.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 25 Nov 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://zipping-suger.github.io/media/icon_hueb8600275123224c14125081487382bb_87041_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://zipping-suger.github.io/project/</link>
    </image>
    
    <item>
      <title>Learning Tool-aware Motion Planning with Analytical Policy Gradient</title>
      <link>https://zipping-suger.github.io/project/apg_nueral_planner/</link>
      <pubDate>Tue, 25 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://zipping-suger.github.io/project/apg_nueral_planner/</guid>
      <description>&lt;p&gt;Collision-free motion generation is a core building block for robot manipulation, yet it remains a significant challenge. Traditional methods like RRT or optimization-based planners (e.g., CuRobo) are often computationally expensive and struggle with real-time requirements in cluttered environments. While emerging Neural Motion Planners promise faster inference, they suffer from compounding errors, poor generalization to new environments, and a heavy reliance on expensive expert demonstrations.&lt;/p&gt;
&lt;p&gt;To address these limitations, this project introduces a &lt;strong&gt;Tool-Aware Neural Motion Planner&lt;/strong&gt; trained via &lt;strong&gt;Analytical Policy Gradient (APG)&lt;/strong&gt;. Developed at the Robotic Systems Lab (RSL) at ETH ZÃ¼rich, this method replaces the complex, multi-step planning pipeline with a single end-to-end network.&lt;/p&gt;
&lt;h3 id=&#34;key-innovations&#34;&gt;Key Innovations&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tool-Awareness:&lt;/strong&gt; Unlike standard neural planners that only check for robot body collisions, our policy explicitly encodes the tool shape as a point cloud. This ensures that the attached tool does not interfere with the environment, a critical requirement for practical manipulation tasks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Analytical Policy Gradient (APG) Fine-tuning:&lt;/strong&gt; The most significant contribution is a fine-tuning stage that &lt;strong&gt;eliminates the need for expert data&lt;/strong&gt;. By leveraging a differentiable simulation with a privileged Signed Distance Field (SDF), we can optimize the policy directly for collision avoidance, target accuracy, and smoothness.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;results-and-impact&#34;&gt;Results and Impact&lt;/h3&gt;
&lt;p&gt;We benchmarked this approach against state-of-the-art planners, demonstrating significant improvements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Generating solutions in just &lt;strong&gt;0.48 seconds&lt;/strong&gt;, compared to 3.46 seconds for CuRobo.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Efficiency:&lt;/strong&gt; Achieved higher success rates (81.7%) than leading neural planners (MPINet) while using only &lt;strong&gt;30%&lt;/strong&gt; of the pre-training data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Adaptation:&lt;/strong&gt; The system can adapt to completely out-of-distribution scenarios (such as cabinets or bins) without collecting new expert demonstrations. By simply sampling random problems and applying APG, the policy learns to navigate new constraints 150x faster than generating expert datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We deployed the policy on a physical Franka Emika arm using a single RealSense depth camera. The system successfully navigated cluttered environments and reacted instantly to dynamic obstacles, highlighting the potential of differentiable simulation for real-world robotic autonomy.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/a3wsjeLreCs&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Manipulation Memory: Learning-Based Trajectory Initialization for Robot Manipulation</title>
      <link>https://zipping-suger.github.io/project/deep_manipulation_memory/</link>
      <pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://zipping-suger.github.io/project/deep_manipulation_memory/</guid>
      <description>&lt;p&gt;Fast and efficient motion planning is crucial for robotic manipulation tasks. While optimization-based methods can generate smooth and feasible trajectories, they are highly sensitive to the initial guess, often leading to slow convergence or failure in complex environments. To address this issue, this research presents Deep Manipulation Memory (DM&amp;amp;M), a neural network designed for learning-based trajectory initialization in robotic manipulation tasks.&lt;/p&gt;
&lt;p&gt;At Flink Robotics, I had the opportunity to bring DM&amp;amp;M from research to real-world applications in logistics. In collaboration with Swiss Post, we tested DM&amp;amp;M on parcel singulation tasks, demonstrating its effectiveness in improving trajectory robustness and reducing computation time. This integration showcased how learning-based trajectory initialization can enhance industrial automation, making robotic systems more efficient and adaptable.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Ni-vkNj6aKE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Framework for Efficient and Robust Dynamic Bin-Picking</title>
      <link>https://zipping-suger.github.io/project/dynamic_bin_picking/</link>
      <pubDate>Fri, 10 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://zipping-suger.github.io/project/dynamic_bin_picking/</guid>
      <description>&lt;p&gt;The efficiency and reliability of robotic bin-picking are crucial as it directly affects the productivity of automated in- dustrial processes. However, traditional bin-picking approaches necessitate prerequisites of static objects and fixed collisions. It leads to limitations in deployment, inefficiency during oper- ation, and unreliability under unforeseen disruptions.&lt;/p&gt;
&lt;p&gt;We aim to achieve dynamic bin-picking, breaking the static assumptions in traditional bin-picking. We present a Dynamic Bin-Picking Framework (DBPF) equipping manip- ulators with strong reactivity to handle dynamic targets and obstacles simultaneously. The horizon-based discrete trajectory optimization solves the optimal action in a close-loop manner to support real-time feasible motion. Given 6D poses from suction planning at each timestep, the optimal target pose is selected with consideration of picking consistency and neural network-based tendency-aware-manipulability scores. Heuristic task-specific designs within modules and the pipeline highly con- tribute to the picking success. Through empirical experiments, our method demonstrates superior performance in terms of success rate, efficiency, and reliability than baseline approaches, showcasing a promising solution for the next generation of robotic bin-picking.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/UGZTbHR1PPE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pushing Potential Field for Object Pile Manipulation with Affordance Learning</title>
      <link>https://zipping-suger.github.io/project/affordance_learning/</link>
      <pubDate>Fri, 10 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://zipping-suger.github.io/project/affordance_learning/</guid>
      <description>
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/n-Xe-Srifxk&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Hierarchical Long-Short Term Safety Control and Its Hyperparameter Optimization</title>
      <link>https://zipping-suger.github.io/project/safe_control/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://zipping-suger.github.io/project/safe_control/</guid>
      <description>&lt;p&gt;Human-robot cooperation has broad application prospects in advanced fields such as service, industry and aerospace. How to ensure that cooperative robots can run safely and efficiently in a dynamic and random environment is an important issue in the context of human-robot cooperation.&lt;/p&gt;
&lt;p&gt;In this research, robot safety avoidance is the starting point to solve the safety problems in human-robot cooperation. The hierarchical long short robot safety algorithm based on path planning and safety control algorithm is studied, and the robot safety obstacle
avoidance in dynamic environment is realized through deep visual perception of the environment. In addition, in order to improve the security and efficiency of the algorithm, an active hyperparameter optimizer for security algorithm is proposed to realize automatic
hyperparameter adjustment.&lt;/p&gt;
&lt;p&gt;The robot safety obstacle avoidance system proposed is successfully deployed on the service robot kinova Gen2 jaco2 robot, and experimental tests are carried out. The experiemnt results show that the proposed system allows robot to dynamically avoid colision during interactions with human while completing different tasks, which are shown in the video attached.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/tuJsENionKQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Learning Latent Object-Centric Representations for Visual-Based Robot Manipulation</title>
      <link>https://zipping-suger.github.io/project/latent_representation/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://zipping-suger.github.io/project/latent_representation/</guid>
      <description>&lt;p&gt;For multi-step robotic manipulation, it is important but challenging to predict the future state of the object conditioned on the applied action, especially from the original sensory observation such as images. Successful robotic manipulation requires an accurate predictive model as well as an intricate understanding of the object-environment interactions from high-dimensional images.&lt;/p&gt;
&lt;p&gt;This paper proposes a latent object-centric representation (LOR) that can encode implicit visual features from raw RGB images into a compact and generalizable representation of the object states suitable for future-state prediction. Based on LOR, LOR dynamic neural network (LOR-DNN) is proposed to simultaneously encode object states and predicts the future states with the applied actions of a robot. The learned LOR-DNN can generalize effectively to new situations and can even directly transfer from simulation to the real world. LOR-DNN can be used to plan action sequences to manipulate the object to achieve the target state, allowing the robot to perform multi-step manipulation tasks such as planar pushing.&lt;/p&gt;
&lt;p&gt;Real-world experiments on pushing tasks demonstrate that the proposed method can achieve a high
success rate on pushing previously unseen objects with diverse shapes and scales, outperforming state-of-the-art model-based and end-to-end methods including baselines that use ground-truth object poses. The proposed approach is an important step toward fully autonomous and generalizable visual-based robotic manipulation.&lt;/p&gt;
&lt;p&gt;Here is a experiment demonstration of the proposed method. Please have a look.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/WwxzAwZbOJU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
  </channel>
</rss>
